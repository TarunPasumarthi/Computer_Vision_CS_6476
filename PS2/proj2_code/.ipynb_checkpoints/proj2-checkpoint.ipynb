{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [HW 2: Projection Matrix and Fundamental Matrix Estimation with RANSAC]()\n",
    "\n",
    "1. Projection Matrix  \n",
    "2. Fundamental Matrix Estimation  \n",
    "3. Fundamental Matrix with RANSAC  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from proj2_code.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 Projection Matrix Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.1 Implement Camera Projection\n",
    "\n",
    "In this initial part, in `projection_matrix.py` you will implement camera projection in the `projection(P, points_3d)` from homogenous world coordinates $X_i = [X_i, Y_i, Z_i, 1]$ to non-homogenous pixel coordinates $x_i, y_i$.\n",
    "\n",
    "It will be helpful to recall the equations to convert to pixel coordinates\n",
    "\n",
    "\\begin{align}\n",
    "x_i = \\frac{p_{11}X_i+p_{12}Y_i + p_{13}Z_i + p_{14}}{p_{31}X_i+p_{32}Y_i + p_{33}Z_i + p_{34}} \\quad y_i = \\frac{p_{21}X_i+p_{22}Y_i + p_{23}Z_i + p_{24}}{p_{31}X_i+p_{32}Y_i + p_{33}Z_i + p_{34}}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import projection_matrix\n",
    "np.set_printoptions(suppress=True) # Suppresses printing in scientific notation\n",
    "\n",
    "from proj2_unit_tests.part1_unit_test import (\n",
    "    verify, \n",
    "    test_projection, \n",
    "    test_objective_func,\n",
    "    test_decompose_camera_matrix,\n",
    "    test_calculate_camera_center,\n",
    "    test_estimate_camera_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test for camera projection:', verify(test_projection))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.2: Objective Function \n",
    "\n",
    "In this part, in `projection_matrix.py` you will implement the objective function `objective_function(x, **kwargs)` that will be passed to `scipy.optimize.least_squares` for minimization with the Levenberg-Marquardt algorithm. The input to this function is a [vectorized](https://en.wikipedia.org/wiki/Vectorization_(mathematics)) camera matrix, the output is the term that gets squared in the objective function and should also be vectorized. Scipy takes care of the squaring + summation part.\n",
    "\n",
    "\\begin{align}\n",
    "\\sum_{i=1}^N ( \\color{purple}{\\hat{\\mathbf{P}}\\mathbf{X}_w^i-\\mathbf{x}^i })^2\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test for objective_function:', verify(test_objective_func))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.3: Estimating the Projection Matrix Given Point Correspondences\n",
    "\n",
    "Initially you will run the optimization to estimate $\\mathbf{P}$ using an initial guess that we provide.\n",
    "\n",
    "### Good initial estimate for $\\mathbf{P}$.\n",
    "\n",
    "Optimizing the reprojection loss using Levenberg-Marquardt requires a good initial estimate for $\\mathbf{P}$. This can be done by having good initial estimates for $\\mathbf{K}$ and $\\mathbf{R}^T$ and $\\mathbf{t}$ which you can multiply to then generate your estimated $\\mathbf{K}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_guess_K = np.array([[ 500,   0, 535],\n",
    "                            [   0, 500, 390],\n",
    "                            [   0,   0,  -1]])\n",
    "\n",
    "initial_guess_R_T = np.array([[ 0.5,   -1,  0],\n",
    "                            [   0,    0, -1],\n",
    "                            [   1,  0.5,  0]])\n",
    "\n",
    "initial_guess_I_t = np.array([[   1,    0, 0, 300],\n",
    "                              [   0,    1, 0, 300],\n",
    "                              [   0,    0, 1,  30]])\n",
    "\n",
    "initial_guess_P = np.matmul(initial_guess_K, np.matmul(initial_guess_R_T, initial_guess_I_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the paths and load the data\n",
    "pts2d_path = '../data/pts2d-pic_b.txt'\n",
    "pts3d_path = '../data/pts3d.txt'\n",
    "img_path   = '../data/pic_b.jpg'\n",
    "\n",
    "points_2d = np.loadtxt(pts2d_path)\n",
    "points_3d = np.loadtxt(pts3d_path)\n",
    "img = load_image(img_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate the projection matrix given corresponding 2D & 3D points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = projection_matrix.estimate_camera_matrix(points_2d, points_3d, initial_guess_P)\n",
    "\n",
    "print('The projection matrix is\\n', P)\n",
    "\n",
    "[projected_2d_pts, residual] = evaluate_points(P, points_2d, points_3d);\n",
    "\n",
    "# residual is the sum of Euclidean distances between actual and projected points\n",
    "print('The total residual is {:f}'.format(residual))\n",
    "visualize_points_image(points_2d, projected_2d_pts, '../data/pic_b.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test for estimate_camera_matrix:', verify(test_estimate_camera_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.4: Decomposing the projection matrix\n",
    "\n",
    "In this part in `projection_matrix.py` you will implement `decompose_camera_matrix(P)` that takes as input the camera matrix $\\mathbf{P}$ and outputs the intrinsic $\\mathbf{K}$ and rotation matrix ${}_c\\mathbf{R}_w$, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test for decomposing projection matrix:', verify(test_decompose_camera_matrix))\n",
    "K, R = projection_matrix.decompose_camera_matrix(P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.5: Calculating Camera Center\n",
    "\n",
    "In this part in `projection_matrix.py` you will implement `calculate_camera_center(P, K, R)` that takes as input the \n",
    "projection $\\mathbf{P}$, intrinsic $\\mathbf{K}$ and extrinsic ${}_c\\mathbf{R}_w$ matrix and outputs the camera position in world coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test for calculating camera center:', verify(test_calculate_camera_center))\n",
    "center = projection_matrix.calculate_camera_center(P, K, R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now visualize the camera center and the camera coordinate system as well as the  the world coordinate system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot3dview(points_3d, center)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.6: Taking Your Own Images and Estimating the Projection Matrix + Camera Pose\n",
    "\n",
    "In this part you will take two images of your fiducial object. If you want to also reuse these images for Part II, keep in mind how to take good images for estimating the Fundamental Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image1_path = '../data/book_img1.jpg'\n",
    "image2_path = '../data/book_img2.jpg'\n",
    "\n",
    "img1 = load_image(image1_path)\n",
    "img2 = load_image(image2_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure your fiducial object and define a coordinate system. Fill out the `points_3d` variable with the 3D point locations of the points you'll use for correspondences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_3d = np.array([[0    , 0,  0],\n",
    "                      [23.5 , 0,  0],\n",
    "                      [0    , 21, 0],\n",
    "                      [0    , 0,  4],\n",
    "                      [23.5 , 0 , 4],\n",
    "                      [23.5 , 21, 4],\n",
    "                      [0    , 21, 4]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for each image, find the 2D pixel locations of your 3D points. Hovering over the image gives you the `x,y` coordinates of your cursor on the image. You can use the lower left side controls to zoom into the image for more precise measurements. Fill out `points2d_img1` with these coordinate values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting image 1\n",
    "fig = plt.figure(); ax = fig.add_subplot(111); ax.imshow(img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points2d_img1 = np.array([[700,  3210],\n",
    "                          [230,  1710],\n",
    "                          [2565, 2410],\n",
    "                          [636,  2878],\n",
    "                          [176,  1465],\n",
    "                          [1555, 1085],\n",
    "                          [2640, 2135]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting image 2\n",
    "fig = plt.figure(); ax = fig.add_subplot(111); ax.imshow(img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points2d_img2 = np.array([[2356, 3210],\n",
    "                          [154, 2169],\n",
    "                          [2794, 1884],\n",
    "                          [2487, 2902],\n",
    "                          [90, 1873],\n",
    "                          [1140, 1073 ],\n",
    "                          [2859, 1577]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our objective function will need to read the measurements you just saved from disk. We need to save this data now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('../data/pts3d_fiducial.npy', points_3d)\n",
    "np.savetxt('../data/pts2d_image1.npy', points2d_img1)\n",
    "np.savetxt('../data/pts2d_image2.npy', points2d_img2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.7: Making your own $\\mathbf{K}$,  $\\mathbf{R}^T$ and $[\\mathbf{I}|\\mathbf{t}]$ estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_guess_K = np.array([[ 500,   0, 2024],\n",
    "                            [   0, 500, 1518],\n",
    "                            [   0,   0,  1]])\n",
    "\n",
    "initial_guess_R_T = np.array([[ 0.5,   -1,  0],\n",
    "                            [   0,    0, -1],\n",
    "                            [   1,  0.5,  0]])\n",
    "\n",
    "initial_guess_I_t = np.array([[   1,    0, 0, -30],\n",
    "                              [   0,    1, 0, -30],\n",
    "                              [   0,    0, 1, 30]])\n",
    "\n",
    "initial_guess_P = np.matmul(initial_guess_K, np.matmul(initial_guess_R_T, initial_guess_I_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the paths and load the data\n",
    "pts2d_path = '../data/pts2d_image1.npy'\n",
    "pts3d_path = '../data/pts3d_fiducial.npy'\n",
    "\n",
    "points_2d = np.loadtxt(pts2d_path)\n",
    "points_3d = np.loadtxt(pts3d_path)\n",
    "img = load_image(image1_path)\n",
    "np.array(img.shape[:2])/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize your estimate for the camera pose relative to the world coordinate system. RGB colors correspond with XYZ (first, second and third coordinate). Be mindful of whether you should be passing `R` or `R.T` in for your rotation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot3dview_with_coordinates(points_3d, initial_guess_I_t[:,3], initial_guess_R_T.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the optimization results given your initial guess. If your initial guess is poor the optimizition **will not** work. You will need to make initial estimates for both the images you took."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P1 = projection_matrix.estimate_camera_matrix(points_2d, points_3d, initial_guess_P)\n",
    "print('The projection matrix is\\n', P1)\n",
    "\n",
    "[projected_2d_pts, residual] = evaluate_points(P1, points_2d, points_3d);\n",
    "print('The total residual is {:f}'.format(residual))\n",
    "visualize_points_image(points_2d, projected_2d_pts, image1_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize your estimate for the camera pose relative to the world coordinate system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the pats and load the data\n",
    "pts2d_path = '../data/pts2d_image2.npy'\n",
    "pts3d_path = '../data/pts3d_fiducial.npy'\n",
    "\n",
    "points_2d = np.loadtxt(pts2d_path)\n",
    "points_3d = np.loadtxt(pts3d_path)\n",
    "img = load_image(image1_path)\n",
    "np.array(img.shape[:2])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_guess_K = np.array([[ 500,   0, 2024],\n",
    "                            [   0, 500, 1518],\n",
    "                            [   0,   0,  1]])\n",
    "\n",
    "initial_guess_R_T = np.array([[ 0.5,   -1,  0],\n",
    "                            [   0,    0, -1],\n",
    "                            [   1,  0.5,  -0.5]])\n",
    "\n",
    "initial_guess_I_t = np.array([[   1,    0, 0, -30],\n",
    "                              [   0,    1, 0, -10],\n",
    "                              [   0,    0, 1, 30]])\n",
    "\n",
    "initial_guess_P = np.matmul(initial_guess_K, np.matmul(initial_guess_R_T, initial_guess_I_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot3dview_with_coordinates(points_3d, initial_guess_I_t[:,3], initial_guess_R_T.T) #change this plot to show the world coordinate system better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P2 = projection_matrix.estimate_camera_matrix(points_2d, points_3d, initial_guess_P)\n",
    "#M = sc.calculate_projection_matrix(points_2d, points_3d)\n",
    "print('The projection matrix is\\n', P2)\n",
    "\n",
    "[projected_2d_pts, residual] = evaluate_points(P2, points_2d, points_3d);\n",
    "print('The total residual is {:f}'.format(residual))\n",
    "visualize_points_image(points_2d, projected_2d_pts, image2_path)\n",
    "#visualize_points(points_2d, projected_2d_pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.8 Visualizing both camera poses in the world coordinate system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K1, R1 = projection_matrix.decompose_camera_matrix(P1)\n",
    "center_1 = projection_matrix.calculate_camera_center(P1, K1, R1);\n",
    "print(center_1)\n",
    "\n",
    "K2, R2 = projection_matrix.decompose_camera_matrix(P2)\n",
    "center_2 = projection_matrix.calculate_camera_center(P2, K2, R2);\n",
    "print(center_2)\n",
    "\n",
    "plot3dview_2_cameras(points_3d, center_1, center_2, R1.T, R2.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 Fundamental Matrix Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part you'll be estimating the fundamental matrix. You'll be using a least squares optimizer from SciPy. (Documentation here: https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.least_squares.html)\n",
    "\n",
    "The least squares optimizer takes an objective function, your variables to optimize, and the points that you want to fit a line to. In this case, the objective function is to minimize the point to line distance, where the line is the projection of a point onto another image by the fundamental matrix, and the point is an actual point of a feature in that image. The variable that you want to optimize would be the 9 values in the 3x3 Fundamental Matrix. The points that you are optimizing over are provided to you, and they are the homogeneous coordinates of corresponding features from two different images of the same scene."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.1 Estimate Fundamental Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the `point_line_distance()` method in fundamental_matrix.py.\n",
    "\n",
    "\\begin{align}\n",
    "    d(line, point) = \\frac{au + bv + c}{\\sqrt{a^2 + b^2}}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj2_unit_tests.test_fundamental_matrix import verify\n",
    "from proj2_unit_tests.test_fundamental_matrix import TestFundamentalMatrix\n",
    "\n",
    "test_fundamental_matrix_stereo = TestFundamentalMatrix()\n",
    "TestFundamentalMatrix.setUp(test_fundamental_matrix_stereo)\n",
    "print(\"test_point_line_distance(): \" + verify(test_fundamental_matrix_stereo.test_point_line_distance))\n",
    "print(\"test_point_line_distance_zero(): \" + verify(test_fundamental_matrix_stereo.test_point_line_distance_zero))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement `signed_point_line_errors()`.\n",
    "\n",
    "Keep in mind that SciPy does the squaring and summing for you, so all you have to do in `signed_point_line_errors()` is return a list of each individual error. So if there are 9 points, you have to calculate the `point_line_distance()` between each pair from $Fx_1$ to $x_0$ and also $F^Tx_0$ to $x_1$, then append all errors to a list, such that you end up returning a list of length 18. SciPy will take the list and square each element and sum everything for you. The red parts in the equation below are the parts you'll need to implement.\n",
    "\n",
    "`signed_point_line_errors()`:\n",
    "\\begin{align}\n",
    "    \\color{red}{d(Fx_1, x_0)}^2 + \\color{red}{d(F^T x_0, x_1)}^2\n",
    "\\end{align}\n",
    "\n",
    "You'll also have to make the call to SciPy's least squares optimizer in the `optimize()` method in least_squares_fundamental_matrix.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj2_unit_tests.test_fundamental_matrix import TestFundamentalMatrix2, TestFundamentalMatrix3\n",
    "\n",
    "print(\"TestFundamentalMatrix():\")\n",
    "print(\"test_signed_point_line_errors(): \" + verify(test_fundamental_matrix_stereo.test_signed_point_line_errors))\n",
    "print(\"test_least_squares_optimize(): \" + verify(test_fundamental_matrix_stereo.test_least_squares_optimize))\n",
    "\n",
    "print(\"TestFundamentalMatrix2():\")\n",
    "test_fundamental_matrix_synthetic = TestFundamentalMatrix2()\n",
    "TestFundamentalMatrix2.setUp(test_fundamental_matrix_synthetic)\n",
    "print(\"test_signed_point_line_errors(): \" + verify(test_fundamental_matrix_synthetic.test_signed_point_line_errors))\n",
    "print(\"test_least_squares_optimize(): \" + verify(test_fundamental_matrix_synthetic.test_least_squares_optimize))\n",
    "\n",
    "print(\"TestFundamentalMatrix3():\")\n",
    "test_fundamental_matrix_real = TestFundamentalMatrix3()\n",
    "TestFundamentalMatrix3.setUp(test_fundamental_matrix_real)\n",
    "print(\"test_signed_point_line_errors(): \" + verify(test_fundamental_matrix_real.test_signed_point_line_errors))\n",
    "print(\"test_least_squares_optimize(): \" + verify(test_fundamental_matrix_real.test_least_squares_optimize))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then run the following cell to find the Fundamental Matrix using least squares. You should see the epipolar lines in the correct places in the image. **You'll need to screenshot this and put it in your report.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Load the data for room images\n",
    "points_2d_pic_a = np.loadtxt('../data/pts2d-pic_a.txt')\n",
    "points_2d_pic_b = np.loadtxt('../data/pts2d-pic_b.txt')\n",
    "img_left = load_image('../data/pic_a.jpg')\n",
    "img_right = load_image('../data/pic_b.jpg')\n",
    "\n",
    "import least_squares_fundamental_matrix as ls\n",
    "\n",
    "F = ls.solve_F(points_2d_pic_a, points_2d_pic_b)\n",
    "print(F)\n",
    "\n",
    "# Draw epipolar lines using the fundamental matrix\n",
    "draw_epipolar_lines(F, img_left, img_right, points_2d_pic_a, points_2d_pic_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.2 Try Fundamental Matrix Estimation Yourself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you're going to take two images yourself and estimate the fundamental matrix between them. To do this, take two images and save them as \"my_image_0.jpg\" and \"my_image_1.jpg\" in the \"/data\" folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data for room images\n",
    "my_img_left = load_image('../data/my_image_0.jpg')\n",
    "my_img_right = load_image('../data/my_image_1.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To collect your own data points, run the following cell and mouse over features in the image and record the x- and y-coordinates. You'll need at least 9 points because we are trying to optimize for 9 variables, one for each element in the 3x3 fundamental matrix. Think about how you can choose good features for estimating the fundamental matrix.\n",
    "\n",
    "Store your points in variable \"my_image_0_pts\" and \"my_image_1_pts\" respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib\n",
    "# plotting image 1\n",
    "image_0 = plt.figure(); image_0_ax = image_0.add_subplot(111); image_0_ax.imshow(my_img_left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_image_0_pts = np.array([[ 669, 1193],\n",
    "                           [1377, 2031],\n",
    "                           [2948, 1806],\n",
    "                           [2310,  498],\n",
    "                           [3132, 2056],\n",
    "                           [ 317, 1680],\n",
    "                           [2289, 1295],\n",
    "                           [2559, 1516],\n",
    "                           [1880, 1618]]) # Record your coordinates here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting image 2\n",
    "image_1 = plt.figure(); image_1_ax = image_1.add_subplot(111); image_1_ax.imshow(my_img_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_image_1_pts = np.array([[ 555, 1336],\n",
    "                           [ 436, 1786],\n",
    "                           [2567, 2158],\n",
    "                           [2478,  870],\n",
    "                           [2973, 2502],\n",
    "                           [ 289, 1533],\n",
    "                           [2265, 1581],\n",
    "                           [2175, 1774],\n",
    "                           [1561, 1737]]) # Record your coordinates here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import two_view_data as two_view_data\n",
    "my_F = ls.solve_F(my_image_0_pts, my_image_1_pts)\n",
    "print(my_F)\n",
    "\n",
    "# Draw epipolar lines using the fundamental matrix\n",
    "draw_epipolar_lines(my_F, my_img_left, my_img_right, my_image_0_pts, my_image_1_pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Fundamental Matrix with RANSAC (Szeliski 6.1.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will automate the process of finding the fundamental matrix for an image pair by using proposed interest points and using RANSAC to robustly find true interest point matches between the two images. We will give you some proposed interest points with an accuracy of approximately 90%. You will learn how to find these proposed interest points later in the class.\n",
    "\n",
    "After we have proposed interest points, then RANSAC will select a random subset of those points, \n",
    "you will call your function from part 2 to calculate the fundamental matrix for those points, \n",
    "and then you will check how many other proposed interest points match this\n",
    "fundamental matrix. Then you will repeat this process and select another subset of points using RANSAC until you find the subset of points that produces the best fundamental matrix with the most\n",
    "matching points. Refer to the lecture slides for the RANSAC workflow. Keep in mind that although the fundamental matrix does not normally have 9 degrees of freedom, the way we are performing the optimization in part 2 does not enforce any constraints on the matrix, and so we need 9 point correspondences to perform the optimization for the 9 variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also find a simple explanation of RANSAC at \n",
    "https://www.mathworks.com/discovery/ransac.html.\n",
    "See section 6.1.4 in the textbook for a more thorough explanation of how RANSAC works.\n",
    "\n",
    "### Part 3.1: RANSAC Iterations\n",
    "Begin by calculating the number of iterations $S$ RANSAC will need to perform to guarentee a given success rate $P$ knowing the number of points included in the sample $k$ and the probability of an individual point being a true match $p$. To derive this formula, consider the following:\n",
    " * the probability that any one point has a true match is $p$\n",
    "  * conversely the probability that any one point is not a match is $1-p$\n",
    "  * the probability that two points are both matches is then $p \\cdot p$\n",
    "  * this can be extendeed to $k$ points, for which the probability that they are all true matches is $p^k$\n",
    " * on the other hand, we want the probability that $k$ points are all true matches to be $P$ (and the probability that they are not to be $1-P$)\n",
    " * by repeatedly sampling $k$ points, we can reduce the probability that all of the samples do not contain $k$ true matches\n",
    " * After $S$ samples we want the probability of failure to equal $1-P$\n",
    " \n",
    "Start by setting up this equality $$1-P = ...$$\n",
    "and then rearange it to write a function to solve for $S$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ransac import calculate_num_ransac_iterations\n",
    "from proj2_unit_tests.test_ransac import test_calculate_num_ransac_iterations\n",
    "\n",
    "P = 0.999\n",
    "k = 9\n",
    "p = 0.90\n",
    "# call their ransac iterations function\n",
    "S = calculate_num_ransac_iterations(P, k, p)\n",
    "# print number of trials they will need to run\n",
    "print('S =', int(S))\n",
    "\n",
    "print(\"Test for calculate_num_ransac_iterations(): \" + verify(test_calculate_num_ransac_iterations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "*put these answers in your report*\n",
    "\n",
    "What is the **minimum** number of RANSAC iterations we would we need to find the fundamental matrix with 99.9% certainty from a set of proposed matches that have a 90% point correspondence accuracy? *Keep in mind that we need at least 9 point correspondences for our optimization to find the fundamental matrix in part 2*\n",
    "\n",
    "One might imagine that if we had more than 9 point correspondences, it would be better to use more of them to solve for the fundamental matrix. Investigate this by finding the number of RANSAC iterations you would need to run for the above situation with 18 points.\n",
    "\n",
    "If our dataset had a lower point correspondence accuracy, say 70%, what is the minimum number of iterations needed to find the fundamental matrix with 99.9% certainty?\n",
    "\n",
    "At the end of this assignment you will be performing RANSAC to find the fundamental matrix for an image pair, and you will want to keep these results in mind when deciding how many iterations to perform. For example, we have shown that you want to use the minimum number of points in order to make it easier to find a sample with no outliers. You will also want to keep the accuracy and probability of success in mind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3.2: RANSAC Implementation\n",
    "Next we will implement the RANSAC algorithm. Remember the steps from the link above:\n",
    " 1. Randomly selecting a subset (k=9) of the data set\n",
    " 1. Fitting a model to the selected subset\n",
    " 1. Determining the number of outliers\n",
    " 1. Repeating steps 1-3 for a prescribed number of iterations\n",
    "\n",
    "For the application of finding true point pair matches and using them to calculate the fundamental matrix, our subset of the data will be the minimum number of point pairs needed to calculate the fundamental matrix.\n",
    "The model we are fitting is the fundamental matrix.\n",
    "Outliers will be found by using the `point_line_distance()` error function from part 2 and thresholding with a certain margin of error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ransac import ransac_fundamental_matrix\n",
    "from proj2_unit_tests import test_ransac\n",
    "\n",
    "points_a = np.load('../data/pointsa.npy')\n",
    "points_b = points_a\n",
    "\n",
    "F, _, _ = ransac_fundamental_matrix(points_a, points_b)\n",
    "print('F= ', F)\n",
    "\n",
    "print(\"Test for ransac_find_inliers(): \" + verify(test_ransac.test_ransac_find_inliers))\n",
    "print(\"Test for ransac_fundamental_matrix(), F matches inliers: \" + verify(test_ransac.test_ransac_fundamental_matrix_error))\n",
    "print(\"Test for ransac_fundamental_matrix(), F matches all points: \" + verify(test_ransac.test_ransac_fundamental_matrix_fit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3.3: Finally we will put it all together\n",
    "\n",
    "The code below will load precomputed feature point correspondence proposals from two images and run your RANSAC calculating the fundamental matrix with your function at each pass. You shouldn't have to implement any new functions for this.\n",
    "\n",
    "This code will display the proposed point correspondences. Note that there are a number of spurious matches. After running your RANSAC it will display the inlier point correspondences and epipolar lines for each feature point according to the fundamental matrix that your code found. You can use these images as a final check. It is possible that some false correspondences will slip through, but most of them should be eliminated, and there should be a clear epipole at the camera center shown in the image. Review the lecture materials if you don't remember the significance of epipolar lines and epipoles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from feature_matching.extraction import get_matches\n",
    "from feature_matching.utils import PIL_resize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Notre Dame\n",
    "image1 = load_image('../data/IMG_4407.jpeg')\n",
    "image2 = load_image('../data/IMG_0788.jpeg')\n",
    "\n",
    "\n",
    "from feature_matching.utils import show_interest_points\n",
    "from feature_matching.utils import show_correspondence_circles, show_correspondence_lines\n",
    "\n",
    "# from feature_matching.extraction import get_matches\n",
    "# pts1, pts2 = get_matches(image1, image2)\n",
    "\n",
    "# load the points from the disk.\n",
    "# the points are already matched. The first row of pts1 matches with first row of pts2.\n",
    "pts1 = np.loadtxt('../data/pts_a.txt')\n",
    "pts2 = np.loadtxt('../data/pts_b.txt')\n",
    "\n",
    "print('Number of matches loaded = ', pts1.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pts_to_visualize = 100\n",
    "num_pts_total = min(pts1.shape[0], pts2.shape[0])\n",
    "pts_to_viz = np.random.choice(np.arange(num_pts_total), num_pts_to_visualize, replace=False)\n",
    "\n",
    "c2 = show_correspondence_lines(image1, image2,\n",
    "                    pts1[pts_to_viz, 0], pts1[pts_to_viz, 1],\n",
    "                    pts2[pts_to_viz, 0], pts2[pts_to_viz, 1],)\n",
    "plt.figure(figsize=(15,20)) \n",
    "plt.title('Proposed Matches')\n",
    "plt.imshow(c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from proj2_code.ransac import ransac_fundamental_matrix\n",
    "\n",
    "F, matches_x0, matches_x1 = ransac_fundamental_matrix(pts1, pts2)\n",
    "print('Fundamental matrix=', F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj2_code.utils import draw_epipolar_lines\n",
    "# Draw the epipolar lines on the images and corresponding matches\n",
    "\n",
    "num_pts_to_visualize = min(50, matches_x0.shape[0])\n",
    "num_pts_total = matches_x0.shape[0]\n",
    "pts_to_viz = np.random.choice(np.arange(num_pts_total), num_pts_to_visualize, replace=False)\n",
    "\n",
    "match_image = show_correspondence_lines(image1, image2,\n",
    "                                   matches_x0[pts_to_viz, 0], matches_x0[pts_to_viz, 1],\n",
    "                                   matches_x1[pts_to_viz, 0], matches_x1[pts_to_viz, 1])\n",
    "plt.figure(figsize=(15,20))\n",
    "plt.title('True Matches')\n",
    "plt.imshow(match_image)\n",
    "\n",
    "\n",
    "draw_epipolar_lines(F, image1, image2, matches_x0, matches_x1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Pose recovery from the fundamental matrix\n",
    "Note: This part is compulsory for grad students. Undergraduate students can attempt this for up to 10 points worth of extra credits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4.1 Recover essential matrix from fundamental matrix\n",
    "Given the fundamental matrix, we can use the camera intrinsics to recover the essential matrix. Refer to the writeup/lecture slides for more details and the final equation.\n",
    "\n",
    "For sake of simplicity, we will use the same calibration matrix for both the cameras. We will reuse the fundamental matrix computed in the previous part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the camera intrinsic matrix\n",
    "K = np.array([[600, 0, 240],\n",
    "              [0, 600, 320],\n",
    "              [0, 0, 1]\n",
    "             ])\n",
    "\n",
    "from proj2_code.recover_rot_translation import recover_E_from_F\n",
    "\n",
    "E = recover_E_from_F(F, K)\n",
    "\n",
    "print('Recovered essential matrix = ', E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj2_unit_tests.test_essential_matrix_decomposition import TestEssentialMatrixDecomposition\n",
    "\n",
    "test_essential_matrix = TestEssentialMatrixDecomposition()\n",
    "test_essential_matrix.setUp()\n",
    "print(\"test_recover_E_from_F(): \" + verify(test_essential_matrix.test_recover_E_from_F))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4.2 Recover relative rotation and translation between camera poses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj2_code.recover_rot_translation import recover_rot_translation_from_E\n",
    "\n",
    "R1, R2, t = recover_rot_translation_from_E(E)\n",
    "\n",
    "print('Rotation candidate #1 = ', R1)\n",
    "print('Rotation candidate #2 = ', R2)\n",
    "print('translation (scale and sign ambiguous) = ', t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj2_unit_tests.test_essential_matrix_decomposition import TestEssentialMatrixDecomposition\n",
    "\n",
    "test_essential_matrix = TestEssentialMatrixDecomposition()\n",
    "test_essential_matrix.setUp()\n",
    "print(\"test_recover_rot_translation_from_E(): \" + verify(test_essential_matrix.test_recover_rot_translation_from_E))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
